{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요 요건\n",
    "\n",
    "이하 예제 코드는 다음 패키지를 필요로 합니다.\n",
    "\n",
    "- pandas\n",
    "- matplotlib\n",
    "- scikit-learn\n",
    "- python-mecab-ko\n",
    "- gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib scikit-learn python-mecab-ko gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token To Vector\n",
    "\n",
    "토큰화된 텍스트 및 그 인덱스도 머신이 바로 이해할 수 있는 형태가 아닙니다.\n",
    "\n",
    "우리는 텍스트를 어떤 공간 좌표 위에 할당하여 그 좌표 벡터가 토큰을 대표(represent)하는 방식을 사용합니다.\n",
    "\n",
    "그 좌표의 위치관계와 거리(유사도)가 곧 단어의 의미와 관계를 함의할 것이라고 믿습니다. \n",
    "\n",
    "텍스트를 벡터화하는 방법은 여러가지 있습니다.\n",
    "- N-gram\n",
    "- Bag-of-words\n",
    "- TF-IDF\n",
    "- GloVE\n",
    "- ... 등등 \n",
    "\n",
    "단어를 벡터화하는 것을 워드 임베딩(Word Embedding)이라 합니다. \n",
    "\n",
    "해당 코드 예제에서는 토큰화된 텍스트로 워드 임베딩(Word Embedding)을 생성하는 방법을 시연합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원-핫 임베딩 (One-hot Embedding)\n",
    "\n",
    "가장 간단한 임베딩입니다. 사전 길이 만큼의 차원을 가지는 0벡터를 만들고, 단어의 인덱스 번호의 값만 1로 설정합니다.\n",
    "\n",
    "값 하나만 1이기 때문에 One-hot, One-hot Vector 라고 부릅니다.\n",
    "\n",
    "어떠한 언어적인 의미도 내포하지 않습니다. 행렬에서 특정 행 또는 열을 선별(look-up)할때 행렬곱에서 사용하는 기능적인 벡터로도 쓰입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"나\", \"엄마\", \"아빠\", \"사과\", \"바나나\"]\n",
    "\n",
    "word2index = {v: i for i, v in enumerate(vocab)}\n",
    "\n",
    "def one_hot_encoding(index, maxlen):\n",
    "    \"\"\"차원이 maxlen이고 index의 위치가 1인 one-hot 벡터를 반환합니다.\"\"\"\n",
    "    tmp = [0] * maxlen\n",
    "    tmp[index] = 1\n",
    "    return tmp\n",
    " \n",
    "\n",
    "for v in vocab:\n",
    "    print(f\"'{v}'의 인덱스: {word2index[v]}\")\n",
    "    print(f\"'{v}'의 원-핫 벡터: {one_hot_encoding(word2index[v], len(vocab))} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "TF-IDF는 단어 빈도(Term Frequency)와 문서 빈도의 역수 (Inverse Document Frequency)를 활용하여 단어와 문서의 관계를 벡터화하는 통계적인 방법입니다.\n",
    "\n",
    "단어가 한 문서에서 자주 등장하지만, 여러 문서에서 만연하게 등장하지 않는다면 중요한 단어일 가능성이 높다는 직관에서 시작합니다.\n",
    "\n",
    "TF-IDF는 다양한 가중치 함수를 가지고 있으며 여기서는 다음과 같은 방식을 예제에서 사용합니다.\n",
    "\n",
    "단어집합 $T$와 문서집합 $D$에 대하여 \n",
    "$$\n",
    "f_{t,d} = \\text{\\{문서 d에서 단어 t의 등장 횟수\\}}\\\\\n",
    "n_t = \\text{\\{단어 t가 등장한 문서의 개수\\}}\\\\\n",
    "\\text{tf}(t, d) = f_{t,d}\\\\\n",
    "\\text{idf}(t) = \\log{\\left(\\frac{|D|}{n_t+1}\\right)}\\\\\n",
    "\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\cdot \\text{idf}(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "docs = [\n",
    "    \"나 는 엄마 가 좋다\",\n",
    "    \"나 는 아빠 가 좋다\",\n",
    "    \"사과 는 과일 입니다\",\n",
    "    \"사과 의 효능 사과 는 건강 에 좋다\",\n",
    "    \"바나나 가 과일 입니다\",\n",
    "]\n",
    "# 사용한 모든 단어\n",
    "vocab = list(set([w for doc in docs for w in doc.split()]))\n",
    "vocab.sort()\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문서의 개수\n",
    "N = len(docs)\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.split().count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        if t in doc:\n",
    "            df += 1\n",
    "    return log(N/(df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)*idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 문서에 대한 TF 매트릭스\n",
    "tf_matrix = []\n",
    "\n",
    "for d in docs:\n",
    "    tmp = []\n",
    "    for t in vocab:\n",
    "        tmp.append(tf(t,d))\n",
    "    tf_matrix.append(tmp)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "\n",
    "pd.DataFrame(tf_matrix, index=[f\"docs #{i}\" for i in range(N)], columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에 대한 IDF\n",
    "idf_list = []\n",
    "\n",
    "for t in vocab:\n",
    "    idf_list.append(idf(t))\n",
    "\n",
    "\n",
    "pd.DataFrame(idf_list, index=vocab, columns=[\"IDF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 매트릭스\n",
    "tfidf_matrix = []\n",
    "\n",
    "for d in docs:\n",
    "    tmp = []\n",
    "    for t in vocab:\n",
    "        tmp.append(tfidf(t,d))\n",
    "    tfidf_matrix.append(tmp)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "\n",
    "pd.DataFrame(tfidf_matrix, index=[f\"docs #{i}\" for i in range(N)], columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과에서 다음을 알 수 있습니다.\n",
    "- 만연하게 등장하는 단어일수록 IDF 값이 작아져 TF-IDF 값이 전체적으로 작아진다 (ex. \"가\", \"는\").\n",
    "- 고유하게 등장하는 단어일수록 IDF 값이 커져 TF-IDF 값이 전체적으로 커진다 (ex. \"건강\", \"바나나\").\n",
    "- 한 문서에서 반복해서 등장하는 단어라면 그 문서에서의 TF-IDF 값이 커진다 (ex. 문서 #3의 \"사과\").\n",
    "\n",
    "우리는 한 단어에 대한 TF-IDF 가중치 리스트를 그 단어에 대한 벡터로 다룰 수 있습니다. 이 예제에서 예를 들어, \"사과\"의 벡터는\n",
    "```\n",
    "[0.0    0.0     0.5108  1.021   0.0]\n",
    "```\n",
    "라고 볼 수 있습니다.\n",
    "\n",
    "-----\n",
    "\n",
    "문서 집합에서 하나의 문서가 하나의 주제만을 다루면서 집합 전체는 다양한 분야를 다룬다면, TF-IDF는 효과적인 워드 임베딩 수단이 될 수 있습니다.\n",
    "\n",
    "밀접한 관계를 가진 단어들은 특정 주제에서 함께 빈도가 높을 것이며, 그런 단어들의 벡터들은 서로 유사하게 (벡터 공간에서 가깝게) 형성될 것이기 때문입니다.\n",
    "\n",
    "단점으로는, 문서의 구성에 따라 TF-IDF의 워드임베딩 품질이 달라지기에 적절하게 문서들을 선정/분할해야 한다는 번거로운 점이 있습니다.\n",
    "\n",
    "또한 문서의 개수에 따라 벡터 차원의 크기가 달라지며, 자주 등장하지 않는 단어의 벡터는 대다수의 문서에서 0값을 가지게 되어 많은 단어와 문서를 다루기에 메모리 공간에 대해 비효율적이라는 단점이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "Word2Vec은 자연어 모델링 문제를 활용하여 단어의 벡터를 학습하는 일련의 방법을 말합니다. 대표적인 방법으로는 크게 두가지가 있습니다.\n",
    "\n",
    "- CBOW\n",
    "- Skip-Gram\n",
    "\n",
    "두 과정 모두 토큰화를 위한 단어 사전 (vocabulary)과 학습에 필요한 말뭉치가 이미 있다고 전제합니다.\n",
    "\n",
    "## CBOW (Continous Bag-of-Words)\n",
    "\n",
    "CBOW는 이웃한 단어를 보고 중심 단어를 예측하는 문제를 해결합니다.\n",
    "\n",
    "이웃한 단어를 입력, 중심단어를 출력(label)으로 하는 자연어 모델링 문제입니다.\n",
    "\n",
    "\n",
    "\n",
    "최종적으로 최종적으로 단어의 벡터를 학습합니다.\n",
    "\n",
    "학습 과정을 도식으로 나타내면 다음과 같습니다.\n",
    "\n",
    "![image.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/CBOW_1.png?raw=true)\n",
    "\n",
    "차원 $V$는 사전의 크기, 차원 $M$은 은닉층의 차원입니다.\n",
    "\n",
    "은닉층과 출력층, 총 2층의 행렬곱(projection) 층으로 이루어져 있습니다.\n",
    "\n",
    "먼저 은닉층부터 살펴봅시다. 각 입력 토큰(원핫 벡터)를 가중치 $W$에 곱합니다.\n",
    "\n",
    "![image-2.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/CBOW_2.png?raw=true)\n",
    "\n",
    "이때 단어 원핫 벡터와 가중치 행렬이 행렬곱한 결과 $V_i$는 행렬의 하나의 행입니다.\n",
    "\n",
    "즉, $V_i$는 그 단어에 대한 벡터 표현 (잠재 표현)이라고 말할 수 있습니다.\n",
    "\n",
    "여기서 학습시키는 가중치 $W$는 곧 단어 벡터의 집합으로 여길 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "우리는 중심 단어를 예측하기 위하여, 이웃한 모든 단어의 벡터표현의 평균을 구합니다.\n",
    "\n",
    "![image-3.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/CBOW_3.png?raw=true)\n",
    "\n",
    "\n",
    "그 후 다음 출력층을 통과하고 softmax 활성화 함수를 적용합니다.\n",
    "\n",
    "![image-4.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/CBOW_4.png?raw=true)\n",
    "\n",
    "이 층의 출력 $\\hat{y}$는 사전에 있는 각 단어에 대하여 중심 단어일 예측 확률 벡터입니다.\n",
    "\n",
    "최종적으로 중심 단어의 원-핫 벡터와 Cross Entropy 손실함수를 적용하여 손실 값을 구한 뒤, 이를 역전파하여 학습을 진행합니다.\n",
    "\n",
    "![image-5.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/CBOW_5.png?raw=true)\n",
    "\n",
    "## Skip-gram\n",
    "\n",
    "CBOW와 반대로 Skip-gram은 중심 단어를 보고 이웃한 단어들을 예측하는 문제를 다룹니다.\n",
    "\n",
    "입력이 중심 단어이고 각 이웃 단어들에 대해서 손실함수를 구한다는 점을 제외하면 내부 원리는 똑같습니다.\n",
    "\n",
    "![image-6.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/CBOW_6.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 Word2Vec 만들기 실습\n",
    "\n",
    "`gensim` 패키지는 Word2Vec을 쉽게 학습할 수 있는 모델을 제공합니다.\n",
    "\n",
    "토큰으로 분할된 텍스트 말뭉치를 입력하면 사전 생성부터 임베딩 학습까지 하나의 호출로 이루어줍니다.\n",
    "\n",
    "이 패키지와 한국어 영화 리뷰 데이터를 이용하여 한국어 Word2Vec 생성을 시연해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from mecab import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "train_data = pd.read_table('ratings.txt')\n",
    "\n",
    "print(\"총 리뷰 개수:\", len(train_data))\n",
    "\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 (missing value) 확인\n",
    "print(\"결측값 유무:\", train_data.isnull().values.any())\n",
    "\n",
    "# 결측값 제거\n",
    "train_data = train_data.dropna(how=\"any\") # null이 존재하는 행 제거\n",
    "\n",
    "print(\"처리 후 결측값 유무:\", train_data.isnull().values.any())\n",
    "\n",
    "print(\"처리 후 리뷰 개수:\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "mecab = MeCab()\n",
    "\n",
    "# 토큰화\n",
    "tokenized_data = []\n",
    "for i, line in enumerate(train_data[\"document\"]):\n",
    "    tokenized_line = mecab.morphs(line)\n",
    "    tokenized_line = [word for word in tokenized_line if word not in stopwords]\n",
    "    tokenized_data.append(tokenized_line)\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"\\r({i+1}/{len(train_data)})줄 처리 완료\", end=\"\")\n",
    "print(\"\\n전부 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 길이 분포 그림\n",
    "print(\"리뷰의 최대 길이\", max([len(review) for review in tokenized_data]))\n",
    "print(\"리뷰의 평균 길이\", sum([len(review) for review in tokenized_data])/len(tokenized_data))\n",
    "\n",
    "plt.hist([len(review) for review in tokenized_data], bins = 50)\n",
    "plt.xlabel(\"length of reviews\")\n",
    "plt.ylabel(\"number of reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 의 Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_data, # 학습 데이터\n",
    "    vector_size=128, # 토큰 잠재벡터 크기\n",
    "    window=5, # 중심단어에서 이웃단어를 선택하는 범위. \n",
    "    min_count=5, # 이 빈도보다 적게 나타나는 단어는 제외함\n",
    "    workers=4, # 멀티쓰레딩 쓰레드 개수. \n",
    "    sg=0 # 0 = CBOW, 1=skip-gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 끝나면 model.wv 에 결과가 저장됩니다.\n",
    "# 완성된 임베딩 매트릭스의 크기 확인\n",
    "print(model.wv.vectors.shape)\n",
    "\n",
    "# 생성된 사전 확인, 토큰 to 인덱스. dict 형 \n",
    "for i, k in enumerate(model.wv.key_to_index):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"token: {k}  index: {model.wv.key_to_index[k]}\")\n",
    "\n",
    "print()\n",
    "# 인덱스 to 토큰. 주의: 리스트형\n",
    "for i in range(10):\n",
    "    print(f\"index: {i}  token: {model.wv.index_to_key[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사한 (벡터를 가지는) 단어 찾기. 단어 - 유사도 쌍 리스트 반환\n",
    "print(\"'영화'와 유사한 단어\")\n",
    "for token, sim in model.wv.most_similar(\"영화\"): print(f\" - {token}, 유사도: {sim}\")\n",
    "\n",
    "print(\"'남자'와 유사한 단어\")\n",
    "for token, sim in model.wv.most_similar(\"남자\"): print(f\" - {token}, 유사도: {sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`영화`와 유사한 단어로 `작품`, `드라마`, `애니메이션` 등이 반환됩니다.\n",
    "\n",
    "이는 주로 문장에서 어떤 단어 대신에 치환되어도 일리가 있는 단어들이 유사한 벡터를 생성하도록 학습되기 때문입니다.\n",
    "\n",
    "\n",
    "다음 코드로 Word2Vec을 저장하고 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('kor_w2v') # 모델 저장\n",
    "loaded_model = Word2Vec.load(\"kor_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 시각화\n",
    "\n",
    "고차원의 임베딩 벡터는 우리의 눈으로 살펴보기에는 너무 복잡합니다.\n",
    "\n",
    "차원감소기법인 T-SNE를 활용하여 2차원으로 감소시켜 단어간의 대략적인 위치관계를 시각화해볼 수 있습니다.\n",
    "\n",
    "![image.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/W2V_visualization.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "# 1500개의 단어만 선별. 시각화\n",
    "n_tokens = 1500\n",
    "X_tsne = tsne.fit_transform(model.wv.vectors[:n_tokens])\n",
    "\n",
    "plt.figure(figsize=[30, 30])\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1],)\n",
    "for i, text in enumerate(model.wv.index_to_key[:n_tokens]):\n",
    "    plt.annotate(text, X_tsne[i])\n",
    "\n",
    "plt.xlim(min(X_tsne[:, 0])*0.8, max(X_tsne[:, 0])*0.8)\n",
    "plt.ylim(min(X_tsne[:, 1])*0.8, max(X_tsne[:, 1])*0.8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
