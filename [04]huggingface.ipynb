{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í—ˆê¹…í˜ì´ìŠ¤ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "ì†Œê·œëª¨ì˜ ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì— ëŒ€í•´ì„œëŠ” ì¼ë°˜ì‚¬ìš©ìë„ í•™ìŠµë¶€í„° ì¶”ë¡ ì˜ ì²˜ìŒë¶€í„° ëê¹Œì§€ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ìµœê·¼ì—ëŠ”, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì˜ ì ‘ê·¼ì´ ìš©ì´í•´ì§€ê³  GPTì™€ ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ (LLM)ì˜ ì„±ê³µìœ¼ë¡œ\n",
    "\n",
    "ê¸°ëŒ€í•˜ëŠ” ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” í•™ìŠµ ë°ì´í„°ì˜ ê·œëª¨ì™€ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ í¬ê¸°ê°€ ì ì°¨ ì»¤ì ¸ê°”ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜, ê³„ì‚° ìì›ì´ ë¶€ì¡±í•œ ëŒ€ë¶€ë¶„ì˜ ì¼ë°˜ ì‚¬ìš©ìëŠ” ëŒ€ê·œëª¨ì˜ ë°ì´í„°ì…‹ê³¼ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì²˜ë¦¬í•œ í›„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ ì¡°ì°¨ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•œ ê°€ì§€ ëŒ€ì•ˆìœ¼ë¡œëŠ”, ëŒ€ê·œëª¨ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë§Œì„ ë‹¤ìš´ë¡œë“œ ë°›ì•„\n",
    "\n",
    "ì¶”ë¡ ë§Œ í•˜ëŠ”ë° ì‚¬ìš©í•˜ê±°ë‚˜ ì†Œê·œëª¨ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ë‹¤ì‹œ í•™ìŠµí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì˜ˆì œ ì½”ë“œì—ì„œëŠ” huggingfaceì˜ transformers íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ ë°›ê³  í™œìš©í•˜ëŠ” ë°©ë²•ì„ ê°„ëµí•˜ê²Œ ì†Œê°œí•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "- transformers\n",
    "- datasets\n",
    "- evaluate\n",
    "- accelerate\n",
    "- sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (4.46.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (0.2.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sympy in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets evaluate accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "ìì—°ì–´ì²˜ë¦¬ì—ì„œ huggingfaceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œë°›ì•„ í™œìš©í•˜ëŠ” ë‘ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. ëª¨ë¸ ì¤‘ì : Model (ê³¼ Tokenizer) ì‚¬ìš©\n",
    "2. íƒœìŠ¤í¬ ì¤‘ì : pipeline ì‚¬ìš©\n",
    "\n",
    "## Model ì‚¬ìš©\n",
    "\n",
    "huggingfaceì˜ Model í´ë˜ìŠ¤ì— ë§ì¶”ì–´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì¼ë°˜ì ì¸ ê²½ìš° ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ëª¨ë¸ì— ë§ëŠ” ì ì ˆí•œ í´ë˜ìŠ¤ë¥¼ ì°¾ì•„ ì´ˆê¸°í™”í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒì€ ì¼ë°˜ëª©ì  ì–¸ì–´ëª¨ë¸ì¸ flan-t5ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì‚¬ìš©í•´ë³´ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\n",
    "\n",
    "**Note:** huggingfaceëŠ” `AutoModel` (ê³¼ `AutoTokenizer`)ë¼ëŠ” ìë™ìœ¼ë¡œ ì ì ˆí•œ í´ë˜ìŠ¤ë¡œ ì´ˆê¸°í™” ì‹œì¼œì£¼ëŠ” ë„ìš°ë¯¸ í´ë˜ìŠ¤ë„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "pipe = None\n",
    "\n",
    "def release():\n",
    "    \"\"\"gpu ë©”ëª¨ë¦¬ë¥¼ ë¹„ìš°ê¸° ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.\"\"\"\n",
    "    global model, tokenizer, pipe\n",
    "    del model, tokenizer, pipe\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    pipe = None\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd446cbbe4dd456ab6a63ecf29bc70d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xaiseung24\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7d7520518b4437a06dafe3315a3945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6261ed056b46b5b4dd11d5b4859332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95801a9943264c52ae42891a6a4fba6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb5188f4bb949c1a9b069162ecf96b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77462a2b1a746dcb747160ad125e4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is the force that pulls objects together.\n"
     ]
    }
   ],
   "source": [
    "text = \"How does the gravity work?\"\n",
    "input_tensor = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "output_tensor = model.generate(input_tensor, do_sample=False, max_length=32)\n",
    "print(tokenizer.decode(output_tensor[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## íŒŒì´í”„ë¼ì¸ ì‚¬ìš©\n",
    "\n",
    "transformers íŒ¨í‚¤ì§€ì—ëŠ” `pipeline()`ì´ë¼ëŠ” ì¶”ìƒì ì¸ ë©”ì†Œë“œë¡œ ì‚¬ì „ì— ì •ì˜ëœ íŒŒì´í”„ë¼ì¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "`pipeline()`ì— í…ìŠ¤íŠ¸ ìƒì„±, í…ìŠ¤íŠ¸ ë¶„ë¥˜, ì¶”ì¶œì  ì§ˆì˜ì‘ë‹µ, ë²ˆì—­ ë“± íƒœìŠ¤í¬ë¥¼ ì§€ì •í•˜ê³  ì´ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ IDë¥¼ ì…ë ¥ìœ¼ë¡œ ì£¼ë©´, ì´ì— ë§ëŠ” `Pipeline` í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•´ì¤ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒë¶€í„°ëŠ” íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ `flan-t5-large`ë¥¼ í™œìš©í•˜ëŠ” ì˜ˆì‹œ ì½”ë“œë“¤ì…ë‹ˆë‹¤. ìœ„ì˜ ì½”ë“œì—ì„œ í–ˆë˜ tokenizerì˜ ì „/í›„ì²˜ë¦¬ë¥¼ pipeline ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•´ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Gravity is the force that pulls objects together.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "release()\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model_id, max_new_tokens=128, device_map=device)\n",
    "\n",
    "print(pipe(\"How does the gravity work?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì¶”ì¶œì  ì§ˆì˜ì‘ë‹µ (Extractive Question-Answering)\n",
    "\n",
    "ì¶”ì¶œì  ì§ˆì˜ì‘ë‹µì€ ë§¥ë½ê³¼ ë§¥ë½ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ë§¥ë½ìœ¼ë¡œë¶€í„° ì¶”ì¶œí•˜ì—¬ ëŒ€ë‹µí•˜ëŠ” íƒœìŠ¤í¬ë¥¼ ë§í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì…ë ¥ìœ¼ë¡œ contextì™€ questionì´ ì£¼ì–´ì§€ë©°, ëª¨ë¸ì€ questionì— ëŒ€í•˜ì—¬ ëŒ€ë‹µì´ ë  ìˆ˜ ìˆëŠ” contextì˜ ì¼ë¶€ë¶„ì„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ë¡œ í—ˆê¹…í˜ì´ìŠ¤í—ˆë¸Œì— ìˆëŠ” í•œêµ­ì–´ ì¶”ì¶œì  QA ëª¨ë¸ `jihoonkimharu/bert-base-klue-mrc-finetuned`ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "pipeline task ì´ë¦„ìœ¼ë¡œëŠ” `question-answering`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a62805f94744468d1e7e1c585ddb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xaiseung24\\.cache\\huggingface\\hub\\models--jihoonkimharu--bert-base-klue-mrc-finetuned. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87199664d9c2496aab66e868db49728c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e5e0071638463a837d8a132627c63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/499 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf4c0ac83844ab0b08c1bd2de992e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c18742685f4345b7018c192463910d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e12870fb264a45b205c0bb0fd54b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8773866891860962, 'start': 244, 'end': 250, 'answer': '17ì„¸ê¸°ì—ëŠ”'}\n",
      "{'score': 0.8648744821548462, 'start': 76, 'end': 82, 'answer': '4,000ë…„'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "release()\n",
    "\n",
    "model_id = \"jihoonkimharu/bert-base-klue-mrc-finetuned\"\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model_id, device_map=device)\n",
    "\n",
    "body_text = \"\"\"ì‚¬ê³¼ë‚˜ë¬´ì˜ ì›ì‚°ì§€ëŠ” ë°œì¹¸ë°˜ë„ë¡œ ì•Œë ¤ì ¸ ìˆìœ¼ë©° B.C. 20ì„¸ê¸° ê²½ì˜ ìŠ¤ìœ„ìŠ¤ í† êµ´ ì£¼ê±°ì§€ì—ì„œ íƒ„í™”ëœ ì‚¬ê³¼ê°€ ë°œêµ´ëœ ê²ƒìœ¼ë¡œ ë³´ì•„ ì„œì–‘ì‚¬ê³¼ëŠ” 4,000ë…„ ì´ìƒì˜ ì¬ë°° ì—­ì‚¬ë¥¼ ê°€ì§„ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤.\n",
    "ê·¸ë¦¬ìŠ¤ ì‹œëŒ€ì—ëŠ” ì¬ë°°ì¢…, ì•¼ìƒì¢…ì„ êµ¬ë¶„í•œ ê¸°ë¡ì´ ìˆê³  ì ‘ëª© ë²ˆì‹ë²•ì´ ì´ë¯¸ ì†Œê°œ ë˜ì–´ ìˆì„ ì •ë„ë¡œ ì¬ë°° ê¸°ìˆ ì´ ì§„ë³´ë˜ì—ˆë‹¤.\n",
    "ë¡œë§ˆì‹œëŒ€ì—ëŠ” Malus ë˜ëŠ” Malumì´ë€ ëª…ì¹­ìœ¼ë¡œ ì¬ë°°ê°€ ì„±í–¥í•˜ì˜€ê³  ê·¸ í›„ 16-17ì„¸ê¸°ì— ê±¸ì³ ìœ ëŸ½ ê°ì§€ì— ì „íŒŒë˜ì—ˆë‹¤.\n",
    "17ì„¸ê¸°ì—ëŠ” ë¯¸êµ­ì— ì „íŒŒë˜ì—ˆê³  20ì„¸ê¸°ì—ëŠ” ì¹ ë ˆ ë“± ë‚¨ë¯¸ ê°êµ­ì— ì „íŒŒë˜ì—ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(pipe({\"context\": body_text, \"question\": \"ë¯¸êµ­ì— ì‚¬ê³¼ê°€ ì „íŒŒëœ ì‹œê¸°ëŠ” ì–¸ì œì¸ê°€?\"}))\n",
    "\n",
    "print(pipe({\"context\": body_text, \"question\": \"ì„œì–‘ ì‚¬ê³¼ê°€ ì—­ì‚¬ëŠ” ì–´ëŠì •ë„ì˜ ì‹œê°„ì¸ê°€?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê°ì • ë¶„ì„ (Sentiment analysis)\n",
    "\n",
    "ê°ì • ë¶„ì„ì€ ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê¸,ë¶€ì • ë˜ëŠ” ë¶„ë…¸, í–‰ë³µ, ê³µí¬ ë“±ì˜ ì–´ë–¤ ê°ì •ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ íŒë‹¨í•˜ëŠ” íƒœìŠ¤í¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "BERT ê¸°ë°˜ ì˜ì–´ ê°ì • ë¶„ì„ ëª¨ë¸ `finiteautomata/bertweet-base-sentiment-analysis`ì„ í™œìš©í•´ë³´ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
    "\n",
    "í•´ë‹¹ ëª¨ë¸ì€ ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê¸ì • (POS), ë¶€ì • (NEG), ì¤‘ë¦½ (NEU)ìœ¼ë¡œ ê°ì • ë¶„ì„ì„ í•˜ì—¬ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "pipeline task ì´ë¦„ìœ¼ë¡œëŠ” `text-classification`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c6a4b9e2db46f39eeec5a62d62ee56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/949 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xaiseung24\\.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-sentiment-analysis. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a34e01dcb84cb68a730e4129b29786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49aec3bf9bbd4272a478993144a7e05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/338 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a9eeef12834b0fa7671070f6165d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f420ae114b4d41b2a2aca30f931147d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3ab7e3112b492e82190ddd2969fdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/22.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43663a0203634976a1e1be41e4e4a088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POS', 'score': 0.9094418287277222}]\n",
      "[{'label': 'NEG', 'score': 0.9702497720718384}]\n",
      "[{'label': 'NEU', 'score': 0.9634673595428467}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "release()\n",
    "\n",
    "model_id = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model_id, device_map=device)\n",
    "\n",
    "print(pipe(\"Never gonna let you down.\")) # ì ˆë•Œ ì‹¤ë§ì‹œí‚¤ì§€ ì•Šì„ê»˜.\n",
    "print(pipe(\"Shut up, I don't wanna hear you.\")) # ë‹¥ì³, ë„ˆí•œí…Œ ë“£ê³ ì‹¶ì§€ ì•Šì•„.\n",
    "print(pipe(\"What time is it?\")) # ëª‡ì‹œì•¼?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë§ˆìŠ¤í¬ ì±„ìš°ê¸° (Fill Mask)\n",
    "\n",
    "ë§ˆìŠ¤í¬ ì±„ìš°ê¸°ëŠ” ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ `[MASK]` ë¡œ ë¹„ì–´ìˆëŠ” ë¶€ë¶„ì— ë“¤ì–´ê°ˆ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ íƒœìŠ¤í¬ëŠ” ì£¼ë¡œ ì‚¬ì „í•™ìŠµ ë‹¨ê³„ì—ì„œ ë‹¤ë£¨ì–´ ìì—°ì–´ì˜ ì´í•´ ìì²´ë¥¼ í•™ìŠµí•˜ëŠ”ë° ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒì€ ìœ ëª…í•œ ì‚¬ì „í•™ìŠµ ìì—°ì–´ ëª¨ë¸ `BERT`ë¥¼ ì‚¬ìš©í•´ë³´ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
    "\n",
    "pipeline task ì´ë¦„ìœ¼ë¡œ `fiil-mask`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df3e782c0b54afba5e5a346accfd88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xaiseung24\\.cache\\huggingface\\hub\\models--google-bert--bert-large-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d928a016dc4e16bda040483938a4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at google-bert/bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f621b91da95d4764b6fa4161f54b52fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eccc56550284aafbdfeba75703cb5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db5c86d2f7042c5bc84c8f2a22dbf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.1278262436389923, 'token': 2417, 'token_str': 'red', 'sequence': \"apple ' s color is red.\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "release()\n",
    "\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=model_id, device_map=device)\n",
    "\n",
    "print(pipe(\"apple's color is [MASK].\", top_k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì‚¬ì „í•™ìŠµ ëª¨ë¸ íŒŒì¸íŠœë‹í•˜ê¸°\n",
    "\n",
    "ìš°ë¦¬ê°€ ì›í•˜ëŠ” íƒœìŠ¤í¬ì— ëŒ€í•´ì„œ ì¸ê³µì§€ëŠ¥ì„ ì ìš©í•˜ê³ ì í•  ë•Œ, ê°•ë ¥í•œ ì‚¬ì „í•™ìŠµ íŒŒë¼ë¯¸í„°ì—ì„œ ì‹œì‘í•œë‹¤ë©´ ì²˜ìŒë¶€í„° í•™ìŠµí•˜ëŠ” ê²½ìš°ì— ë¹„í•´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ìœ¼ë©´ì„œ ì‹œê°„ì„ ì•„ë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì™œëƒí•˜ë©´, ëŒ€ê·œëª¨ ë§ë­‰ì¹˜ë¥¼ í†µí•´ì„œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì€ ê·¸ ì–¸ì–´ì— ëŒ€í•´ì„œ í’ë¶€íˆ ì´í•´í•˜ê³  ìˆê¸° ë•Œë¬¸ì— êµ¬ì²´ì ì¸ íƒœìŠ¤í¬ì— ëŒ€í•´ ìƒˆë¡­ê²Œ í›ˆë ¨ì„ í•´ì¤¬ì„ ë•Œ ì´ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë¹ ë¥´ê²Œ ì–»ì–´ë‚´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë ‡ê²Œ ëŒ€ê·œëª¨ ë§ë­‰ì¹˜ì— ì‚¬ì „í•™ìŠµ í›„ íŒŒì¸íŠœë‹ì— ì‚¬ìš©ë˜ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ì„ **Foundation Model**ì´ë¼ ë¶€ë¦…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒì„ ì˜í™” ê¸ë¶€ì • ë¦¬ë·°ì— ëŒ€í•˜ì—¬ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì œì…ë‹ˆë‹¤.\n",
    "\n",
    "*Note: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í–ˆê¸°ì— ê¸°ëŒ€í•˜ëŠ” ì„±ëŠ¥ë³´ë‹¤ ì¡°ê¸ˆ ì €ì¡°í•©ë‹ˆë‹¤.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    )\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "release()\n",
    "\n",
    "\n",
    "model_id = \"distilbert-base-cased\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, device_map = device, )#torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "raw_dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "del raw_dataset[\"unsupervised\"]\n",
    "\n",
    "# í‰ê°€ ì§€í‘œ ì •ì˜\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions,references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ ì˜ˆì‹œ\n",
    "raw_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e1f67210d64889b7e5eb2312f5b65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©\n",
    "small_train_dataset = raw_dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "small_eval_dataset = raw_dataset[\"test\"].shuffle(seed=42).select(range(400))\n",
    "\n",
    "# ì „ì²˜ë¦¬\n",
    "max_length = 48\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "small_train_dataset = small_train_dataset.map(tokenize_fn, batched=True)\n",
    "small_eval_dataset = small_eval_dataset.map(tokenize_fn, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì¤€ë¹„ - Trainer ë³€ìˆ˜\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir=\"distilbert_imdb\",\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    optim=\"adafactor\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=trainer_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491e1e3c056a4fa7935fd06b12385897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6964366436004639,\n",
       " 'eval_model_preparation_time': 0.0019,\n",
       " 'eval_accuracy': 0.51,\n",
       " 'eval_runtime': 9.558,\n",
       " 'eval_samples_per_second': 41.85,\n",
       " 'eval_steps_per_second': 0.732}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ìµœì´ˆ ì„±ëŠ¥ ì²´í¬.\n",
    "trainer.evaluate()\n",
    "# í‰ê°€ê°€ ì•ˆë˜ê³  ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ \n",
    "# per_device_train_batch_sizeì™€ per_device_eval_batch_sizeì˜ ê°’ì„ ì¤„ì¸ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì •í™•ë„ê°€ 0.5ì— ê°€ê¹ê²Œ ë‚˜ì˜¨ë‹¤ë©´, ê±°ì˜ ë¬´ì‘ìœ„ë¡œ ì˜ˆì¸¡í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ê°€ ë¶ˆëŸ¬ì˜¨ ì‚¬ì „í•™ìŠµëœ BERT ëª¨ë¸ì€ ì˜í™”ì˜ ê¸ë¶€ì • ë¦¬ë·°ë¥¼ ìœ„í•´ í›ˆë ¨ëœ ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ íŒŒì¸íŠœë‹ì„ í†µí•´ ì˜í™” ê¸ë¶€ì •ì— ëŒ€í•´ì„œ í›ˆë ¨í•˜ë©° ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26722fb54b34e52b233ab848221c1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e05f5572a85498c8641c9b343f3f6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5197383761405945, 'eval_model_preparation_time': 0.0019, 'eval_accuracy': 0.7675, 'eval_runtime': 10.936, 'eval_samples_per_second': 36.577, 'eval_steps_per_second': 0.64, 'epoch': 1.0}\n",
      "{'train_runtime': 227.4126, 'train_samples_per_second': 8.795, 'train_steps_per_second': 0.141, 'train_loss': 0.6244255304336548, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=0.6244255304336548, metrics={'train_runtime': 227.4126, 'train_samples_per_second': 8.795, 'train_steps_per_second': 0.141, 'total_flos': 24837637248000.0, 'train_loss': 0.6244255304336548, 'epoch': 1.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•™ìŠµ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d6f40df38648b7bd5f6f34787bcd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5197383761405945,\n",
       " 'eval_model_preparation_time': 0.0019,\n",
       " 'eval_accuracy': 0.7675,\n",
       " 'eval_runtime': 10.7883,\n",
       " 'eval_samples_per_second': 37.077,\n",
       " 'eval_steps_per_second': 0.649,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•™ìŠµ í›„ í‰ê°€\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ì—ëŠ” ì •í™•ë„ê°€ 75% ì •ë„ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì„ ì €ì¥í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./distilbert_imdb/latest\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DistilBertForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    )\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "release()\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬\n",
    "model_path = \"./distilbert_imdb/latest\"\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (model_id ëŒ€ì‹  ë””ë ‰í† ë¦¬ë¥¼ ë„£ê¸°)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, device_map = device, )#torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer ì¬ì •ì˜\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir=\"distilbert_imdb\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=trainer_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea24b2e7d954fd8aad6c88f389dbb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5197383761405945,\n",
       " 'eval_model_preparation_time': 0.0016,\n",
       " 'eval_accuracy': 0.7675,\n",
       " 'eval_runtime': 11.6996,\n",
       " 'eval_samples_per_second': 34.189,\n",
       " 'eval_steps_per_second': 0.598}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í‰ê°€\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
