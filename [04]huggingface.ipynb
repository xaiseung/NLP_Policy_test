{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 허깅페이스 사전학습 모델 사용\n",
    "\n",
    "소규모의 데이터셋과 모델에 대해서는 일반사용자도 학습부터 추론의 처음부터 끝까지 수행하는 것이 가능합니다.\n",
    "\n",
    "최근에는, 대규모 데이터셋의 접근이 용이해지고 GPT와 같은 대규모 언어 모델 (LLM)의 성공으로\n",
    "\n",
    "기대하는 성능을 얻기 위해 사용하는 학습 데이터의 규모와 모델 파라미터의 크기가 점차 커져갔습니다.\n",
    "\n",
    "그러나, 계산 자원이 부족한 대부분의 일반 사용자는 대규모의 데이터셋과 대규모 모델을 처리한 후 학습시키는 것 조차 불가능합니다.\n",
    "\n",
    "한 가지 대안으로는, 대규모의 데이터셋으로 사전학습된 모델 가중치만을 다운로드 받아\n",
    "\n",
    "추론만 하는데 사용하거나 소규모의 데이터셋으로 다시 학습할 수도 있습니다.\n",
    "\n",
    "이 예제 코드에서는 huggingface의 transformers 패키지를 사용하여 사전학습 모델을 다운로드 받고 활용하는 방법을 간략하게 소개합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패키지 설치\n",
    "\n",
    "- transformers\n",
    "- datasets\n",
    "- evaluate\n",
    "- accelerate\n",
    "- sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (4.46.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (0.2.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sympy in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets evaluate accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 불러오기\n",
    "\n",
    "자연어처리에서 huggingface에서 모델을 다운로드받아 활용하는 두가지 방법이 있습니다.\n",
    "\n",
    "1. 모델 중점: Model (과 Tokenizer) 사용\n",
    "2. 태스크 중점: pipeline 사용\n",
    "\n",
    "## Model 사용\n",
    "\n",
    "huggingface의 Model 클래스에 맞추어서 가중치를 로드합니다.\n",
    "\n",
    "일반적인 경우 사용하고자 하는 모델에 맞는 적절한 클래스를 찾아 초기화해주어야 합니다.\n",
    "\n",
    "다음은 일반목적 언어모델인 flan-t5를 다운로드하고 사용해보는 예제입니다.\n",
    "\n",
    "**Note:** huggingface는 `AutoModel` (과 `AutoTokenizer`)라는 자동으로 적절한 클래스로 초기화 시켜주는 도우미 클래스도 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "pipe = None\n",
    "\n",
    "def release():\n",
    "    \"\"\"gpu 메모리를 비우기 위한 함수입니다.\"\"\"\n",
    "    global model, tokenizer, pipe\n",
    "    del model, tokenizer, pipe\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    pipe = None\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd446cbbe4dd456ab6a63ecf29bc70d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xaiseung24\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7d7520518b4437a06dafe3315a3945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6261ed056b46b5b4dd11d5b4859332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95801a9943264c52ae42891a6a4fba6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb5188f4bb949c1a9b069162ecf96b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77462a2b1a746dcb747160ad125e4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is the force that pulls objects together.\n"
     ]
    }
   ],
   "source": [
    "text = \"How does the gravity work?\"\n",
    "input_tensor = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "output_tensor = model.generate(input_tensor, do_sample=False, max_length=32)\n",
    "print(tokenizer.decode(output_tensor[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이프라인 사용\n",
    "\n",
    "transformers 패키지에는 `pipeline()`이라는 추상적인 메소드로 사전에 정의된 파이프라인을 불러올 수 있습니다.\n",
    "\n",
    "`pipeline()`에 텍스트 생성, 텍스트 분류, 추출적 질의응답, 번역 등 태스크를 지정하고 이를 지원하는 모델 ID를 입력으로 주면, 이에 맞는 `Pipeline` 클래스를 반환해줍니다.\n",
    "\n",
    "다음부터는 파이프라인으로 `flan-t5-large`를 활용하는 예시 코드들입니다. 위의 코드에서 했던 tokenizer의 전/후처리를 pipeline 내부에서 자동으로 수행해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Gravity is the force that pulls objects together.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "release()\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model_id, max_new_tokens=128, device_map=device)\n",
    "\n",
    "print(pipe(\"How does the gravity work?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추출적 질의응답 (Extractive Question-Answering)\n",
    "\n",
    "추출적 질의응답은 맥락과 맥락에 대한 질문이 주어졌을 때, 질문에 대한 대답을 맥락으로부터 추출하여 대답하는 태스크를 말합니다.\n",
    "\n",
    "입력으로 context와 question이 주어지며, 모델은 question에 대하여 대답이 될 수 있는 context의 일부분을 반환해야 합니다.\n",
    "\n",
    "모델로 허깅페이스허브에 있는 한국어 추출적 QA 모델 `jihoonkimharu/bert-base-klue-mrc-finetuned`을 사용할 것입니다.\n",
    "\n",
    "pipeline task 이름으로는 `question-answering`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a62805f94744468d1e7e1c585ddb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xaiseung24\\.cache\\huggingface\\hub\\models--jihoonkimharu--bert-base-klue-mrc-finetuned. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87199664d9c2496aab66e868db49728c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e5e0071638463a837d8a132627c63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/499 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf4c0ac83844ab0b08c1bd2de992e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c18742685f4345b7018c192463910d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e12870fb264a45b205c0bb0fd54b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8773866891860962, 'start': 244, 'end': 250, 'answer': '17세기에는'}\n",
      "{'score': 0.8648744821548462, 'start': 76, 'end': 82, 'answer': '4,000년'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "release()\n",
    "\n",
    "model_id = \"jihoonkimharu/bert-base-klue-mrc-finetuned\"\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model_id, device_map=device)\n",
    "\n",
    "body_text = \"\"\"사과나무의 원산지는 발칸반도로 알려져 있으며 B.C. 20세기 경의 스위스 토굴 주거지에서 탄화된 사과가 발굴된 것으로 보아 서양사과는 4,000년 이상의 재배 역사를 가진 것으로 추정된다.\n",
    "그리스 시대에는 재배종, 야생종을 구분한 기록이 있고 접목 번식법이 이미 소개 되어 있을 정도로 재배 기술이 진보되었다.\n",
    "로마시대에는 Malus 또는 Malum이란 명칭으로 재배가 성향하였고 그 후 16-17세기에 걸쳐 유럽 각지에 전파되었다.\n",
    "17세기에는 미국에 전파되었고 20세기에는 칠레 등 남미 각국에 전파되었다.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(pipe({\"context\": body_text, \"question\": \"미국에 사과가 전파된 시기는 언제인가?\"}))\n",
    "\n",
    "print(pipe({\"context\": body_text, \"question\": \"서양 사과가 역사는 어느정도의 시간인가?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감정 분석 (Sentiment analysis)\n",
    "\n",
    "감정 분석은 입력된 텍스트에 대한 긍,부정 또는 분노, 행복, 공포 등의 어떤 감정이 포함되어 있는지 판단하는 태스크입니다.\n",
    "\n",
    "BERT 기반 영어 감정 분석 모델 `finiteautomata/bertweet-base-sentiment-analysis`을 활용해보는 예시입니다.\n",
    "\n",
    "해당 모델은 입력 텍스트에 대한 긍정 (POS), 부정 (NEG), 중립 (NEU)으로 감정 분석을 하여 예측한 클래스를 출력합니다.\n",
    "\n",
    "pipeline task 이름으로는 `text-classification`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c6a4b9e2db46f39eeec5a62d62ee56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/949 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xaiseung24\\.cache\\huggingface\\hub\\models--finiteautomata--bertweet-base-sentiment-analysis. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a34e01dcb84cb68a730e4129b29786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49aec3bf9bbd4272a478993144a7e05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/338 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a9eeef12834b0fa7671070f6165d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f420ae114b4d41b2a2aca30f931147d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3ab7e3112b492e82190ddd2969fdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/22.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43663a0203634976a1e1be41e4e4a088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POS', 'score': 0.9094418287277222}]\n",
      "[{'label': 'NEG', 'score': 0.9702497720718384}]\n",
      "[{'label': 'NEU', 'score': 0.9634673595428467}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "release()\n",
    "\n",
    "model_id = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model_id, device_map=device)\n",
    "\n",
    "print(pipe(\"Never gonna let you down.\")) # 절때 실망시키지 않을께.\n",
    "print(pipe(\"Shut up, I don't wanna hear you.\")) # 닥쳐, 너한테 듣고싶지 않아.\n",
    "print(pipe(\"What time is it?\")) # 몇시야?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마스크 채우기 (Fill Mask)\n",
    "\n",
    "마스크 채우기는 주어진 텍스트에서 `[MASK]` 로 비어있는 부분에 들어갈 단어를 예측합니다.\n",
    "\n",
    "이 태스크는 주로 사전학습 단계에서 다루어 자연어의 이해 자체를 학습하는데 사용합니다.\n",
    "\n",
    "다음은 유명한 사전학습 자연어 모델 `BERT`를 사용해보는 예시입니다.\n",
    "\n",
    "pipeline task 이름으로 `fiil-mask`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df3e782c0b54afba5e5a346accfd88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xaiseung24\\.conda\\envs\\general\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xaiseung24\\.cache\\huggingface\\hub\\models--google-bert--bert-large-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d928a016dc4e16bda040483938a4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at google-bert/bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f621b91da95d4764b6fa4161f54b52fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eccc56550284aafbdfeba75703cb5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db5c86d2f7042c5bc84c8f2a22dbf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.1278262436389923, 'token': 2417, 'token_str': 'red', 'sequence': \"apple ' s color is red.\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "release()\n",
    "\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=model_id, device_map=device)\n",
    "\n",
    "print(pipe(\"apple's color is [MASK].\", top_k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전학습 모델 파인튜닝하기\n",
    "\n",
    "우리가 원하는 태스크에 대해서 인공지능을 적용하고자 할 때, 강력한 사전학습 파라미터에서 시작한다면 처음부터 학습하는 경우에 비해 더 좋은 성능을 얻으면서 시간을 아낄 수 있습니다.\n",
    "\n",
    "왜냐하면, 대규모 말뭉치를 통해서 사전학습된 모델은 그 언어에 대해서 풍부히 이해하고 있기 때문에 구체적인 태스크에 대해 새롭게 훈련을 해줬을 때 이 이해를 바탕으로 좋은 성능을 빠르게 얻어내기 때문입니다.\n",
    "\n",
    "이렇게 대규모 말뭉치에 사전학습 후 파인튜닝에 사용되는 대규모 언어모델을 **Foundation Model**이라 부릅니다.\n",
    "\n",
    "다음을 영화 긍부정 리뷰에 대하여 사전학습된 모델을 파인튜닝하는 간단한 예제입니다.\n",
    "\n",
    "*Note: 빠른 테스트를 위해 데이터셋의 일부만 사용했기에 기대하는 성능보다 조금 저조합니다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    )\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "release()\n",
    "\n",
    "\n",
    "model_id = \"distilbert-base-cased\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, device_map = device, )#torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "raw_dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "del raw_dataset[\"unsupervised\"]\n",
    "\n",
    "# 평가 지표 정의\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions,references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 예시\n",
    "raw_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e1f67210d64889b7e5eb2312f5b65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋의 일부만 사용\n",
    "small_train_dataset = raw_dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "small_eval_dataset = raw_dataset[\"test\"].shuffle(seed=42).select(range(400))\n",
    "\n",
    "# 전처리\n",
    "max_length = 48\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "small_train_dataset = small_train_dataset.map(tokenize_fn, batched=True)\n",
    "small_eval_dataset = small_eval_dataset.map(tokenize_fn, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 준비 - Trainer 변수\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir=\"distilbert_imdb\",\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    optim=\"adafactor\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=trainer_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491e1e3c056a4fa7935fd06b12385897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6964366436004639,\n",
       " 'eval_model_preparation_time': 0.0019,\n",
       " 'eval_accuracy': 0.51,\n",
       " 'eval_runtime': 9.558,\n",
       " 'eval_samples_per_second': 41.85,\n",
       " 'eval_steps_per_second': 0.732}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최초 성능 체크.\n",
    "trainer.evaluate()\n",
    "# 평가가 안되고 오류가 발생하면 \n",
    "# per_device_train_batch_size와 per_device_eval_batch_size의 값을 줄인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도가 0.5에 가깝게 나온다면, 거의 무작위로 예측한다는 뜻입니다.\n",
    "\n",
    "우리가 불러온 사전학습된 BERT 모델은 영화의 긍부정 리뷰를 위해 훈련된 것이 아니기 때문입니다.\n",
    "\n",
    "이제 파인튜닝을 통해 영화 긍부정에 대해서 훈련하며 정확도를 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26722fb54b34e52b233ab848221c1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e05f5572a85498c8641c9b343f3f6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5197383761405945, 'eval_model_preparation_time': 0.0019, 'eval_accuracy': 0.7675, 'eval_runtime': 10.936, 'eval_samples_per_second': 36.577, 'eval_steps_per_second': 0.64, 'epoch': 1.0}\n",
      "{'train_runtime': 227.4126, 'train_samples_per_second': 8.795, 'train_steps_per_second': 0.141, 'train_loss': 0.6244255304336548, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=0.6244255304336548, metrics={'train_runtime': 227.4126, 'train_samples_per_second': 8.795, 'train_steps_per_second': 0.141, 'total_flos': 24837637248000.0, 'train_loss': 0.6244255304336548, 'epoch': 1.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d6f40df38648b7bd5f6f34787bcd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5197383761405945,\n",
       " 'eval_model_preparation_time': 0.0019,\n",
       " 'eval_accuracy': 0.7675,\n",
       " 'eval_runtime': 10.7883,\n",
       " 'eval_samples_per_second': 37.077,\n",
       " 'eval_steps_per_second': 0.649,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 후 평가\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막에는 정확도가 75% 정도 나타납니다.\n",
    "\n",
    "모델을 저장해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./distilbert_imdb/latest\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DistilBertForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    )\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "release()\n",
    "\n",
    "# 저장된 모델 디렉토리\n",
    "model_path = \"./distilbert_imdb/latest\"\n",
    "\n",
    "# 저장된 모델 불러오기 (model_id 대신 디렉토리를 넣기)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, device_map = device, )#torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer 재정의\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir=\"distilbert_imdb\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=trainer_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea24b2e7d954fd8aad6c88f389dbb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5197383761405945,\n",
       " 'eval_model_preparation_time': 0.0016,\n",
       " 'eval_accuracy': 0.7675,\n",
       " 'eval_runtime': 11.6996,\n",
       " 'eval_samples_per_second': 34.189,\n",
       " 'eval_steps_per_second': 0.598}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
