{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG를 이용하여 국방백서를 알려주는 간단한 챗봇 만들기\n",
    "\n",
    "최근 자연어처리 분야에서 검색 증강 생성 (Retrieval-Augmented Generation, RAG)은 각광받는 기술입니다.\n",
    "\n",
    "RAG는 사용자가 질문을 보내면, 사전에 만들어둔 데이터베이스에서 검색하여\n",
    "\n",
    "검색한 내용을 사용자 입력에 붙여서 (Augment) 모델에게 함께 전달합니다. \n",
    "\n",
    "\n",
    "이하 예제 코드는 VectorDB 라이브러리 ChromaDB와 LLM 고수준 프레임워크 Ollama와 Langchain를 이용하여,\n",
    "- PDF 파일로 부터 텍스트를 추출하고 이를 쪼개어서 문서 집합을 생성해봅니다.\n",
    "- ~~생성한 문서집합으로부터 VectorDB를 구축한다.~~\n",
    "  - 미리 생성된 VectorDB를 사용합니다.\n",
    "- LLM과 VectorDB 검색모듈을 연결하여 국방백서에 특화된 챗봇을 제작해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "torch는 이미 다운로드 받았다고 가정합니다. (colab은 이미 설치되어 있습니다.)\n",
    "\n",
    "**(colab의 경우) 앞으로 진행하기 전에 런타임 유형을 변경하시는 것을 추천드립니다!**\n",
    "\n",
    "![image.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/colab_session.png?raw=true)\n",
    "\n",
    "바꾸지 않아도 실행은 되지만, (CPU) 8분 vs (GPU) 24초로 크게 차이가 나게 됩니다.\n",
    "\n",
    "\n",
    "ollama 설치\n",
    "- 빠른 LLM 추론을 위한 프레임워크입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 패키지\n",
    "- pymupdf\n",
    "- ollama\n",
    "- chromadb\n",
    "- huggingface-hub\n",
    "- sentence-transformers\n",
    "- transformers\n",
    "- langchain\n",
    "- langchain-community\n",
    "- langchain-ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf ollama chromadb huggingface-hub sentence-transformers transformers langchain langchain-community langchain-ollama gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습자료 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1WN1efc4LjCX4GO4uRtiRGAhTIC6fH7mk\n",
    "!tar -xzvf \"[06]materials.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama에 모델 등록하기\n",
    "\n",
    "Ollama는 대규모 언어 모델 (LLM) 추론을 로컬에서 빠르게 실행할 수 있게 해주는 도구입니다.\n",
    "\n",
    "제공한 학습자료에서 `./models/bllossom3b` 에는 우리가 사용할 자연어 모델 가중치(파라미터)들이 있습니다. \n",
    "\n",
    "Ollama에서 사용하기 위해서는 해당 모델을 등록해야 합니다.\n",
    "\n",
    "```\n",
    "ollama create bllossom3b -f ./models/bllossom3b/Modelfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama 시작\n",
    "!nohup ollama serve &\n",
    "# bllossom3b 모델 등록\n",
    "!ollama create bllossom3b -f ./models/bllossom3b/Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama에 등록한 모델을 Langchain 프레임워크에 연결하여, 자유롭게 대화해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Ollama 를 이용해 로컬에서 LLM 실행\n",
    "## llama3-ko-instruct 모델 다운로드는 Ollama 사용법 참조\n",
    "\n",
    "model_id = \"bllossom3b\"\n",
    "model = ChatOllama(model=model_id, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.invoke(\"안녕하세요?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bllossom3b`는 한국어에 튜닝된 공개 대규모 언어모델이며, 뒤의 `3b` (3 Billions)는 모델의 크기, 즉 가중치의 개수를 의미합니다.\n",
    "\n",
    "최근 모델 기조에 비하면 모델크기가 작은편이고, 특정 도메인에 한정된 추가 학습이 이루어지지 않았기 때문에 큰 성능을 기대하기는 어렵지만,\n",
    "\n",
    "예제를 시연하는데에는 적합한 크기입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF 데이터 (국방백서) 전처리하기\n",
    "\n",
    "이제 검색 증강 생성 (RAG)를 위한 데이터베이스를 구축해봅시다.\n",
    "\n",
    "먼저, 검색에 사용할 말뭉치가 필요합니다. 우리가 해결하고자 하는 문제에서 도움이 되는 지식을 포함한 텍스트들이여야 합니다.\n",
    "\n",
    "말뭉치는 .txt 파일부터, .pdf, .docx, .html 등등을 사용할 수 있습니다.\n",
    "\n",
    "해당 예제에서는 2022년 국방백서 pdf 파일을 이용하여 RAG를 위한 데이터베이스를 구축해볼 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# PyMuPDFLoader 을 이용해 PDF 파일 로드\n",
    "loader = PyMuPDFLoader(\"2022 국방백서.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# 전처리\n",
    "for page in pages:\n",
    "    page.page_content = page.page_content.replace(\"\\n\", \" \").replace(\"  \", \" \").replace(\". \", \".\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pages`변수는 각 페이지를 나누어 저장해둔 리스트입니다.\n",
    "\n",
    "테스트로 `pages[2]`를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 각 페이지를 짧은 문장으로 쪼개는 과정입니다.\n",
    "\n",
    "짧은 문장으로 나누어지고 나면, 이것이 DB에 각각 저장되는 단위, 문서가 됩니다.\n",
    "\n",
    "- `chunk_size`: 자르는 문서의 크기입니다.\n",
    "- `chunk_overlap`: 문서를 자를 때 인접한 두 문서가 해당 개수만큼 겹치게 자릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 페이지의 텍스트들을 더 작은 단위로 자르기\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=32,\n",
    ")\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서의 개수\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트로 `docs[0]`을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 벡터 저장소 생성\n",
    "\n",
    "벡터 저장소 (Vector Store, VectorDB)란 텍스트, 이미지, 음성, 센서데이터 등 다양한 종류의 데이터들을 벡터로 변환하여 색인 (indexing)한 뒤\n",
    "\n",
    "색인된 벡터를 바탕으로 빠르게 검색할 수 있게 해주는 모듈입니다.\n",
    "\n",
    "우리가 만들 것은 국방백서에 대한 질문이 입력으로 들어왔을 때, 질문에 관련된 정보를 빠르게 찾아내는 벡터저장소를 만들고자 하는 것입니다.\n",
    "\n",
    "이하 코드는 `bge-m3` 모델을 이용하여 문서를 벡터화하고 ChromaDB를 이용하여 벡터 저장소를 만드는 예제입니다.\n",
    "\n",
    "(실행하면 오래 걸리므로, 미리 만들어진 결과물을 사용할 것입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "# 문장을 임베딩으로 변환하고 벡터 저장소에 저장\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-m3',\n",
    "    model_kwargs={'device':device},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "# 벡터 저장소 경로 설정\n",
    "## 현재 경로에 'vectorstore' 경로 생성\n",
    "vectorstore_path = 'vectorstore'\n",
    "os.makedirs(vectorstore_path, exist_ok=True)\n",
    "\n",
    "# 벡터 저장소 생성 및 저장\n",
    "vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=vectorstore_path)\n",
    "#vectorstore = Chroma(embedding_function=embeddings, persist_directory=vectorstore_path)\n",
    "\n",
    "# 벡터스토어 데이터를 디스크에 저장\n",
    "vectorstore.persist()\n",
    "print(\"Vectorstore created and persisted\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 만들어진 벡터 저장소 불러오기\n",
    "\n",
    "- 처음 실행시에는 문서 임베딩 모델을 다운로드 받는 시간이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 저장된 VectorStore 로드\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-m3',\n",
    "    model_kwargs={'device':device},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "vectorstore_path = 'vectorstore'\n",
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=vectorstore_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 국방백서 챗봇 파이프라인 구축하기\n",
    "\n",
    "## 언어모델 불러오기 + 검색 모듈 초기화\n",
    "\n",
    "Ollama로 등록한 `bllossom3b` 모델을 불러오고, 벡터 저장소를 검색모듈로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Ollama 를 이용해 로컬에서 LLM 실행\n",
    "## llama3-ko-instruct 모델 다운로드는 Ollama 사용법 참조\n",
    "\n",
    "model = ChatOllama(model=\"bllossom3b\", temperature=0)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 파이프라인 구축\n",
    "\n",
    "- 대화용 프롬프트를 생성합니다.\n",
    "  - 중괄호롤 묶인 키워드 (예: `{context}`)가 프롬프트의 빈칸(placeholder)가 되어, 입력을 키워드에 대응시켜 이 자리를 채우도록 할 수 있습니다.\n",
    "- 프롬프트에 검색모듈의 검색 결과와 사용자의 입력을 대응시키도록 연결합니다.\n",
    "- 이를 모델에 연결하고 표준 출력을 사용하도록 합니다.\n",
    "- invoke() 메소드를 사용하여, 챗봇을 이용해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# Prompt 템플릿 생성\n",
    "template = '''친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하세요. 모든 대답은 한국어(Korean)으로 대답하세요.\n",
    "다음 문맥(context)는 질문에 도움이 될 수도 있고 아닐 수도 있습니다. 문맥 내애서 북한의 내용인지 남한의 내용인지 잘 구분하세요. 문맥을 바탕으로 알 수 없다면 모르겠다고 답변하세요.\n",
    "\n",
    "context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 검색된 문서 텍스트 전처리\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n\\n'.join([f\"## Docs{i+1}\\n{d.page_content}\" for i, d in enumerate(docs)])\n",
    "\n",
    "# 검색 체인 - 검색 모듈과 사용자 입력을 정해진 placeholder에 배정한다.\n",
    "retrieve_chain = {'context': retriever | format_docs, 'question': RunnablePassthrough()} | prompt\n",
    "# RAG Chain 연결\n",
    "rag_chain = (\n",
    "    retrieve_chain\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챗봇 파이프라인이 만들어졌습니다. 다음과 같이 질문을 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 실행\n",
    "query = \"국제 사회의 안보 불확실성이 증대된 이유가 무엇입니까?\"\n",
    "print(\"Query:\", query)\n",
    "print(\"Answer:\")\n",
    "\n",
    "async for chunk in rag_chain.astream(query):\n",
    "  print(chunk, end =\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "우리의 질문에 대해서 문서 검색이 어떻게 이루어졌고 모델이 받는 입력이 들어갔는지 보기위해\n",
    "\n",
    "RAG 검색 체인만 실행해 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieve_chain.invoke(query).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "또 다른 질문 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"주한미군에 대해서 알려주세요.\"\n",
    "print(\"Query:\", query)\n",
    "print(\"Answer:\")\n",
    "\n",
    "async for chunk in rag_chain.astream(query):\n",
    "  print(chunk, end =\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieve_chain.invoke(query).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "모를 것 같은 질문으로 넣어보고 어떻게 대답하는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"한산대첩에서 사용한 우리나라의 진법이 무엇인지 아시나요?\"\n",
    "print(\"Query:\", query)\n",
    "print(\"Answer:\")\n",
    "\n",
    "async for chunk in rag_chain.astream(query):\n",
    "  print(chunk, end =\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieve_chain.invoke(query).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
