{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 사용하기\n",
    "\n",
    "주어진 텍스트를 머신에게 이해시키려면 먼저 어떤 단위로 다룰 것인지를 정해야 합니다.\n",
    "\n",
    "이러한 단위로 쪼개진 것을 토큰(Token)이라고 하며, 텍스트를 토큰화하는 것을 토크나이저 (Tokenizer)라고 합니다.\n",
    "\n",
    "토큰은 띄어쓰기, 단어 단위로 이루어질 수도 있으며, (한글의 경우) 형태소 단위로 이루어집니다.\n",
    "\n",
    "해당 코드는 영어에서는 단어 단위 토크나이저, 한글에서는 형태소 단위 토크나이저를 소개합니다.\n",
    "\n",
    "\n",
    "### 패키지 다운로드\n",
    "\n",
    "해당 강의의 모든 코드는 Pytorch 패키지를 [홈페이지](https://pytorch.org/get-started/locally/)에서 이미 적절한 방식으로 다운로드 받았다고 가정합니다.\n",
    "\n",
    "- pandas\n",
    "- spacy (다국어 토크나이저)\n",
    "- python-mecab-ko (한국어 형태소 토크나이저)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas spacy python-mecab-ko\n",
    "# Spacy Pipeline download\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어 토큰화\n",
    "\n",
    "spacy 클래스를 활용하여 다음과 같이 영어 텍스트를 토큰화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "en_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "print(tokenize_en(en_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 토큰화\n",
    "\n",
    "한글 형태소 토큰화에는 KoNLPy와 MeCab-ko를 주로 사용하며, 여기서는 Mecab-Ko 구현 패키지 python-mecab-ko를 사용해봅시다.\n",
    "\n",
    "MeCab-ko 패키지는 일본의 형태소 분석을 위해 만들어진 텍스트 분할 라이브러리 MeCab을 한국어에 맞게 변경한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mecab import MeCab\n",
    "\n",
    "ko_text = \"추운 겨울에는 따뜻한 커피와 티를 마셔야지요.\"\n",
    "\n",
    "mecab = MeCab()\n",
    "\n",
    "print(mecab.morphs(ko_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 집합 (vocabulary)생성\n",
    "\n",
    "\"네이버 영화 리뷰 분류하기\" 데이터를 이용하여 사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 파일 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "data = pd.read_table('ratings.txt')\n",
    "print(f\"전체 샘플의 수: {len(data)}\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 일부만 사용합시다\n",
    "sample_data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "# 한글, 영어, 숫자 및 ASCII 특수문자, 공백을 제외하고 삭제합니다.\n",
    "def extract_ko_en_only(s: str):\n",
    "    return re.sub(r\"[^\\x20-\\x7Eㄱ-ㅎㅏ-ㅣ가-힣]+\", \"\", s)\n",
    "\n",
    "sample_data.loc[:, \"document\"] = sample_data[\"document\"].apply(extract_ko_en_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어 정의\n",
    "\n",
    "자주 등장하지만 분석에는 큰 도움이 되지 않는 형태소들을 토큰화 후 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트 예시\n",
    "stopwords = [\"의\", \"가\", \"이\", \"은\", \"는\", \"을\", \"를\", \"이\", \"도\", \"으로\", \"로\", \"하\", \"다\", \"게\", \"음\", \"입니다\", \"습니다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MeCab()\n",
    "tokenized=[]\n",
    "for sentence in sample_data['document']:\n",
    "    temp = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp = [tok for tok in temp if not tok in stopwords] #불용어 제거\n",
    "    tokenized.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 예시\n",
    "for tok_list in tokenized[:5]:\n",
    "    print(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter를 이용하여 토큰 등장 빈도수를 세봅시다.\n",
    "counter = Counter()\n",
    "\n",
    "for tok_list in tokenized:\n",
    "    counter.update(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter의 크기 = 등장한 단어의 수\n",
    "print(len(counter))\n",
    "# 빈도가 많은 10개의 단어 (와 빈도)를 출력해봅니다.\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어처리에서는 **토큰** 하나에 대하여 **벡터** 하나를 할당합니다.\n",
    "\n",
    "즉 우리가 <ins>고려하는 토큰 종류가 많아질수록</ins>, 머신이 <ins>기억해야 하는 벡터의 개수</ins>가 증가합니다.\n",
    "\n",
    "따라서 머신의 성능을 고려하여 단어 집합 (Vocabulary)의 크기를 제한하고, 사전에 제외된 단어 및 형태소를 위한 토큰 (`<unk>`)을 정의합니다.\n",
    "\n",
    "또 이러한 방식의 장점은, 실전에서 학습 데이터에서 등장하지 않았던 단어가 나타나더라도 오류없이 대응할 수 있다는 것입니다.\n",
    "\n",
    "최종적으로 단어집합은 **단어에서 인덱스(번호)**, **인덱스에서 단어간의 매핑**으로 구성되며, 인덱스 번호는 추후 벡터집합 (행렬)의 위치 (행)을 가리키는데 사용됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 사전으로 vocabulary를 정의합니다.\n",
    "word_to_index = {}\n",
    "word_to_index[\"<unk>\"] = 0 # 제외된 단어를 위한 토큰을 vocabulary에 추가합니다.\n",
    "word_to_index[\"<pad>\"] = 1 # 추후에 토큰의 빈칸을 나타낼 때 사용하는 토큰입니다.\n",
    "\n",
    "#가장 자주 등장한 498개의 단어만 추가하여 크기가 총 500인 사전을 만듭시다.\n",
    "mc_word = [c for c, _ in counter.most_common(498)]\n",
    "\n",
    "#enumerate는 순서가 있는 자료형을 입력으로 넣으면 (인덱스, 원소)를 반환합니다.\n",
    "word_to_index.update({key: idx + 2 for idx, key in enumerate(mc_word)})\n",
    "\n",
    "# 확인\n",
    "for key in list(word_to_index.keys())[:10]:\n",
    "    print(f\"단어:\\t{key}\\t인덱스:{word_to_index[key]}\")\n",
    "\n",
    "# 인덱스에서 단어로 복원하는 사전을 만듭니다.\n",
    "index_to_word = list(word_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 리스트에서 정수(index) 리스트로 변환합니다.\n",
    "encoded = []\n",
    "for line in tokenized:\n",
    "    index_list = []\n",
    "    for tok in line:\n",
    "        if tok in word_to_index:\n",
    "            index_list.append(word_to_index[tok])\n",
    "        else:\n",
    "            index_list.append(word_to_index[\"<unk>\"])\n",
    "    encoded.append(index_list)\n",
    "\n",
    "# 확인해봅시다.\n",
    "print(tokenized[0])\n",
    "print(encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩 (padding)\n",
    "\n",
    "우리가 사용하는 문장의 길이가 제각각이며, 따라서 문장당 토큰의 개수도 달라질 수 있습니다.\n",
    "\n",
    "이는 딥러닝 모델이 여러 문장을 동시에 처리 및 학습할 때 병렬화하는 걸림돌이 됩니다.\n",
    "\n",
    "때문에 문장의 길이를 미리 맞춰주는 처리를 하는데, 의미 없는 토큰을 문장 뒤에 추가하는 과정을 패딩이라고 합니다.\n",
    "\n",
    "이 예제에서는 모든 문장을 가장 긴 문장의 길이로 전부 맞춰주는 예제입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 토큰 길이\n",
    "max_len = max([len(l) for l in encoded])\n",
    "\n",
    "print(f\"최대 길이: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩\n",
    "for line in encoded:\n",
    "    line += [word_to_index[\"<pad>\"]]*(max_len-len(line))\n",
    "\n",
    "# 확인\n",
    "print(encoded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
