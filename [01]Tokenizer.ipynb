{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 사용하기\n",
    "\n",
    "주어진 텍스트를 머신에게 이해시키려면 먼저 어떤 단위로 다룰 것인지를 정해야 합니다.\n",
    "\n",
    "이러한 단위로 쪼개진 것을 토큰(Token)이라고 하며, 텍스트를 토큰화하는 것을 토크나이저 (Tokenizer)라고 합니다.\n",
    "\n",
    "토큰은 띄어쓰기, 단어 단위로 이루어질 수도 있으며, (한글의 경우) 형태소 단위로 이루어집니다.\n",
    "\n",
    "해당 코드는 영어에서는 단어 단위 토크나이저, 한글에서는 형태소 단위 토크나이저를 소개합니다.\n",
    "\n",
    "\n",
    "### 패키지 다운로드\n",
    "\n",
    "해당 강의의 모든 코드는 Pytorch 패키지를 [홈페이지](https://pytorch.org/get-started/locally/)에서 이미 적절한 방식으로 다운로드 받았다고 가정합니다.\n",
    "\n",
    "- pandas\n",
    "- spacy (다국어 토크나이저)\n",
    "- python-mecab-ko (한국어 형태소 토크나이저)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas spacy python-mecab-ko\n",
    "# Spacy Pipeline download\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어 토큰화\n",
    "\n",
    "spacy 클래스를 불러와 다음과 같이 토큰화를 진행합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "en_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "print(tokenize_en(en_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 토큰화\n",
    "\n",
    "한글 형태소 토큰화에는 KoNLPy와 MeCab-ko를 주로 사용하며, 여기서는 Mecab-Ko 구현 패키지 python-meCab-ko를 사용해봅시다.\n",
    "\n",
    "MeCab-ko 패키지는 일본의 형태소 분석을 위해 만들어진 텍스트 분할 라이브러리 MeCab을 한국어에 맞게 변경한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mecab import MeCab\n",
    "\n",
    "ko_text = \"추운 겨울에는 따뜻한 커피와 티를 마셔야지요.\"\n",
    "\n",
    "mecab = MeCab()\n",
    "\n",
    "print(mecab.morphs(ko_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 집합 (vocabulary)생성\n",
    "\n",
    "\"네이버 영화 리뷰 분류하기\" 데이터를 이용하여 사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수: 200000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 파일 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "data = pd.read_table('ratings.txt')\n",
    "print(f\"전체 샘플의 수: {len(data)}\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 일부만 사용합시다\n",
    "sample_data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xaiseung24\\AppData\\Local\\Temp\\ipykernel_2732\\872727614.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_data[\"document\"] = sample_data[\"document\"].apply(extract_ko_en_only)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "# 한글, 영어, 숫자 및 ASCII 특수문자, 공백을 제외하고 삭제합니다.\n",
    "def extract_ko_en_only(s: str):\n",
    "    return re.sub(r\"[^\\x20-\\x7Eㄱ-ㅎㅏ-ㅣ가-힣]+\", \"\", s)\n",
    "\n",
    "sample_data[\"document\"] = sample_data[\"document\"].apply(extract_ko_en_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어 정의\n",
    "\n",
    "자주 등장하지만 분석에는 큰 도움이 되지 않는 형태소들을 토큰화 후 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트 예시\n",
    "stopwords = [\"의\", \"가\", \"이\", \"은\", \"는\", \"을\", \"를\", \"이\", \"도\", \"으로\", \"로\", \"하\", \"다\", \"게\", \"음\", \"입니다\", \"습니다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MeCab()\n",
    "tokenized=[]\n",
    "for sentence in sample_data['document']:\n",
    "    temp = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp = [tok for tok in temp if not tok in stopwords] #불용어 제거\n",
    "    tokenized.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ']\n",
      "['디자인', '배우', '학생', ',', '외국', '디자이너', '와', '그', '들', '일군', '전통', '통해', '발전', '해', '문화', '산업', '부러웠', '는데', '.', '사실', '우리', '나라', '에서', '그', '어려운', '시절', '에', '끝', '까지', '열정', '지킨', '노라노', '같', '전통', '있', '어', '저', '와', '같', '사람', '들', '꿈', '꾸', '고', '이뤄나갈', '수', '있', '다는', '것', '에', '감사', '합니다', '.']\n",
      "['폴리스', '스토리', '시리즈', '1', '부터', '뉴', '까지', '버릴', '께', '하나', '없', '.', '.', '최고', '.']\n",
      "['와', '.', '.', '연기', '진짜', '개', '쩔', '구나', '.', '.', '지루', '할거', '라고', '생각', '했', '는데', '몰입', '해서', '봤', '.', '.', '그래', '이런', '진짜', '영화', '지']\n",
      "['안개', '자욱', '한', '밤하늘', '에', '떠', '있', '초승달', '같', '영화', '.']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 예시\n",
    "for tok_list in tokenized[:5]:\n",
    "    print(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter를 이용하여 토큰 등장 빈도수를 세봅시다.\n",
    "counter = Counter()\n",
    "\n",
    "for tok_list in tokenized:\n",
    "    counter.update(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3364\n",
      "[('.', 932), ('영화', 371), ('고', 267), ('에', 198), ('한', 182), ('!', 155), ('있', 151), ('보', 146), (',', 145), ('좋', 122)]\n"
     ]
    }
   ],
   "source": [
    "# Counter의 크기 = 등장한 단어의 수\n",
    "print(len(counter))\n",
    "# 빈도가 많은 10개의 단어 (와 빈도)를 출력해봅니다.\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어처리에서는 토큰 하나에 대하여 벡터 하나를 할당합니다.\n",
    "\n",
    "즉 여러분이 고려하는 토큰 종류가 많아질수록, 머신이 기억해야 하는 벡터의 개수가 증가합니다.\n",
    "\n",
    "따라서 머신의 성능을 고려하여 단어 집합 (Vocabulary)의 크기를 제한하고, 사전에 제외된 단어 및 형태소를 위한 토큰 (`<unk>`)을 정의합니다.\n",
    "\n",
    "또 이러한 방식의 장점은, 실전에서 학습 데이터에서 등장하지 않았던 단어가 나타나더라도 오류없이 대응할 수 있다는 것입니다.\n",
    "\n",
    "최종적으로 단어집합은 단어에서 인덱스(번호), 인덱스에서 단어간의 매핑으로 구성되며, 인덱스 번호는 추후 벡터집합 (행렬)의 위치 (행)을 가리키는데 사용됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어:\t<unk>\t인덱스:0\n",
      "단어:\t<pad>\t인덱스:1\n",
      "단어:\t.\t인덱스:2\n",
      "단어:\t영화\t인덱스:3\n",
      "단어:\t고\t인덱스:4\n",
      "단어:\t에\t인덱스:5\n",
      "단어:\t한\t인덱스:6\n",
      "단어:\t!\t인덱스:7\n",
      "단어:\t있\t인덱스:8\n",
      "단어:\t보\t인덱스:9\n"
     ]
    }
   ],
   "source": [
    "# 파이썬 사전으로 vocabulary를 정의합니다.\n",
    "word_to_index = {}\n",
    "word_to_index[\"<unk>\"] = 0 # 제외된 단어를 위한 토큰을 vocabulary에 추가합니다.\n",
    "word_to_index[\"<pad>\"] = 1 # 추후에 토큰의 빈칸을 나타낼 때 사용하는 토큰입니다.\n",
    "\n",
    "#가장 자주 등장한 498개의 단어만 추가하여 크기가 총 500인 사전을 만듭시다.\n",
    "mc_word = [c for c, _ in counter.most_common(498)]\n",
    "\n",
    "#enumerate는 순서가 있는 자료형을 입력으로 넣으면 (인덱스, 원소)를 반환합니다.\n",
    "word_to_index.update({key: idx + 2 for idx, key in enumerate(mc_word)})\n",
    "\n",
    "# 확인\n",
    "for key in list(word_to_index.keys())[:10]:\n",
    "    print(f\"단어:\\t{key}\\t인덱스:{word_to_index[key]}\")\n",
    "\n",
    "# 인덱스에서 단어로 복원하는 사전을 만듭니다.\n",
    "index_to_word = list(word_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ']\n",
      "[211, 58, 9, 4, 86, 60, 83, 15, 28, 37]\n"
     ]
    }
   ],
   "source": [
    "# 토큰 리스트에서 정수(index) 리스트로 변환합니다.\n",
    "encoded = []\n",
    "for line in tokenized:\n",
    "    index_list = []\n",
    "    for tok in line:\n",
    "        if tok in word_to_index:\n",
    "            index_list.append(word_to_index[tok])\n",
    "        else:\n",
    "            index_list.append(word_to_index[\"<unk>\"])\n",
    "    encoded.append(index_list)\n",
    "\n",
    "# 확인해봅시다.\n",
    "print(tokenized[0])\n",
    "print(encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩 (padding)\n",
    "\n",
    "우리가 사용하는 문장의 길이가 제각각이며, 따라서 문장당 토큰의 개수도 달라질 수 있습니다.\n",
    "\n",
    "이는 딥러닝 모델이 여러 문장을 동시에 처리 및 학습할 때 병렬화하는 걸림돌이 됩니다.\n",
    "\n",
    "때문에 문장의 길이를 미리 맞춰주는 처리를 하는데, 의미 없는 토큰을 문장 뒤에 추가하는 과정을 패딩이라고 합니다.\n",
    "\n",
    "이 예제에서는 모든 문장을 가장 긴 문장의 길이로 전부 맞춰주는 예제입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이: 66\n"
     ]
    }
   ],
   "source": [
    "# 최대 토큰 길이\n",
    "max_len = max([len(l) for l in encoded])\n",
    "\n",
    "print(f\"최대 길이: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[211, 58, 9, 4, 86, 60, 83, 15, 28, 37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 패딩\n",
    "for line in encoded:\n",
    "    line += [word_to_index[\"<pad>\"]]*(max_len-len(line))\n",
    "\n",
    "# 확인\n",
    "print(encoded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
