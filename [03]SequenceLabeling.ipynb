{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 시퀀스 레이블링\n",
    "\n",
    "네이버 영화 리뷰 데이터를 이용해서 긍부정 분류를 해봅시다.\n",
    "\n",
    "각 리뷰마다 해당 리뷰가 긍정인 경우 1, 부정인 경우 0으로 표기되어 있습니다.\n",
    "\n",
    "해당 데이터 활용하여 긍부정 분류를 시연해봅시다.\n",
    "\n",
    "## 패키지 설치\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- python-mecab-ko\n",
    "- tqdm\n",
    "- scikit-learn\n",
    "- gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib python-mecab-ko tqdm scikit-learn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from mecab import MeCab\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 온라인에서 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 파일에서 로드하기\n",
    "train_data = pd.read_table(\"ratings_train.txt\")\n",
    "test_data = pd.read_table(\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(\"훈련용 리뷰 개수:\", len(train_data))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 데이터는 id, document, label 열로 이루어져 있습니다.\n",
    "\n",
    "document에 대한 긍,부정 열이 label입니다. label은 긍정=1, 부정=0으로 이루어져 있으며 이진 분류 모델을 예측해야 하는 값입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 정제하기\n",
    "\n",
    "먼저 텍스트 파일을 전처리해봅시다. 한글과 공백만 남기고 나머지 문자를 제거해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def extract_ko(text):\n",
    "    # 예외(ex. Null) 값 무시\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # 정규표현식\n",
    "    return re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", text)\n",
    "\n",
    "train_data[\"document\"] = train_data[\"document\"].apply(extract_ko)\n",
    "\n",
    "\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복 데이터를 제거해봅시다. 먼저, nunique() 메소드로 중복 데이터가 있는지 확인해봅시다. nunique()는 고유한(unique) 데이터의 개수(n)를 반환합니다 (null값은 제외합니다). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "train_data[\"document\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 리뷰 데이터가 150,000개인데, 해당 출력값이 143,682라는 것은 중복된 데이터가 약 6,400개 있다는 것입니다. 해당 데이터들을 제거해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop_duplicates(subset=[\"document\"])\n",
    "\n",
    "print(\"총 훈련 샘플 수: \", len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리뷰중에 Null 값이 있는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "train_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`True`가 나온다면 Null 값을 가진 샘플이 있다는 뜻입니다. 어느 열, 어느 행에 있는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 어느 열에 몇개 있는지\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# 해당 행 출력\n",
    "train_data.loc[train_data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null 값을 가진 샘플을 제거합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Null 샘플 제거\n",
    "train_data = train_data.dropna(how=\"any\")\n",
    "\n",
    "\n",
    "# 확인\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한, 원본 데이터 또는 전처리된 데이터에서 빈 문자열 `\"\"` 또는 공백만이 있는 데이터가 생성될 수도 있습니다. 이를 제거해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 양 옆 공백을 제거했을 때 (strip) 빈 문자열이 나오는 데이터의 인덱스 리스트 \n",
    "train_data[train_data[\"document\"].str.strip()==\"\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 해당하는 행 제거 drop()\n",
    "train_data = train_data.drop(\n",
    "    train_data[train_data[\"document\"].str.strip()==\"\"].index\n",
    ")\n",
    "\n",
    "print(\"전처리 후 총 훈련 샘플 수: \", len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터에서 똑같은 과정을 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"document\"] = test_data[\"document\"].apply(extract_ko)\n",
    "test_data = test_data.drop_duplicates(subset=[\"document\"])\n",
    "test_data = test_data.dropna(how=\"any\")\n",
    "test_data = test_data.drop(\n",
    "    test_data[test_data[\"document\"].str.strip()==\"\"].index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 (형태소 분할)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
    "\n",
    "tokenizer = MeCab()\n",
    "\n",
    "train_len = len(train_data['document'])\n",
    "test_len = len(test_data['document'])\n",
    "\n",
    "tokenized_train=[]\n",
    "for i, sentence in enumerate(train_data['document']):\n",
    "    temp = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp = [tok for tok in temp if not tok in stopwords] #불용어 제거\n",
    "    tokenized_train.append(temp)\n",
    "    print(f\"[({i}\\{train_len})]\\r\", end=\"\")\n",
    "print(\"\\ntrain data done.\")\n",
    "\n",
    "tokenized_test=[]\n",
    "for i, sentence in enumerate(test_data['document']):\n",
    "    temp = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp = [tok for tok in temp if not tok in stopwords] #불용어 제거\n",
    "    tokenized_test.append(temp)\n",
    "    print(f\"[({i}\\{test_len})]\\r\", end=\"\")\n",
    "print(\"\\ntest data done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 집합 생성 및 토큰 임베딩\n",
    "\n",
    "gensim 라이브러리를 이용하여 단어 집합과 Word2Vec을 동시에 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 128\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    sentences=tokenized_train, # 학습 데이터\n",
    "    vector_size=emb_size, # 토큰 잠재벡터 크기\n",
    "    window=5, # 중심단어에서 이웃단어를 선택하는 범위. \n",
    "    min_count=5, # 이 빈도보다 적게 나타나는 단어는 제외함\n",
    "    workers=4, # 멀티쓰레딩 쓰레드 개수. \n",
    "    sg=1 # 0 = CBOW, 1=skip-gram\n",
    ")\n",
    "\n",
    "# 사전을 형성합니다. 토큰 -> 인덱스\n",
    "word2index = {\"<unk>\": 0, \"<pad>\": 1}\n",
    "word2index.update({k: word2vec.wv.key_to_index[k]+2 for k in word2vec.wv.key_to_index})\n",
    "# 인덱스 -> 토큰\n",
    "index2word = list(word2index.keys())\n",
    "# 각 토큰에 대한 벡터를 저장한 행렬입니다.\n",
    "# <unk> 토큰과 <pad>에 대해서 무작위 벡터를 추가해봅시다.\n",
    "embedding_w = word2vec.wv.vectors\n",
    "\n",
    "unk_pad_emb = np.random.normal(0, 1/np.sqrt(emb_size), size=[2, emb_size])\n",
    "embedding_w = np.concatenate([unk_pad_emb, embedding_w], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분할된 텍스트에서 인덱스로 변환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequences(x_data, word2index):\n",
    "    # 입력이 배치로 들어오지 않는 경우를 예외처리\n",
    "    if isinstance(x_data[0], str):\n",
    "        x_data = [x_data]\n",
    "\n",
    "    encoded_x_data = []\n",
    "    for sent in x_data:\n",
    "        encoded_x_data.append(\n",
    "            [word2index[tok] if tok in word2index else word2index[\"<unk>\"] for tok in sent]\n",
    "        )\n",
    "    return encoded_x_data\n",
    "\n",
    "\n",
    "def sequences_to_text(x_data, index2word):\n",
    "    # 입력이 배치로 들어오지 않는 경우를 예외처리\n",
    "    if isinstance(x_data[0], int):\n",
    "        x_data = [x_data]\n",
    "\n",
    "    encoded_x_data = []\n",
    "    for sent in x_data:\n",
    "        encoded_x_data.append(\n",
    "            [index2word[idx] for idx in sent]\n",
    "        )\n",
    "    return encoded_x_data\n",
    "\n",
    "encoded_train = text_to_sequences(tokenized_train, word2index)\n",
    "encoded_test = text_to_sequences(tokenized_test, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train[0], encoded_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(l) for l in encoded_train]+[len(l) for l in encoded_test])\n",
    "\n",
    "print(f\"최대 길이: {max_len}\")\n",
    "\n",
    "def pad_truncate(x_data, max_len, pad_idx=word2index[\"<pad>\"]):\n",
    "    \"\"\"max len보다 짧으면 패딩하고 길면 자릅니다.\"\"\"\n",
    "    res = np.ones([len(x_data), max_len], dtype=int) * pad_idx\n",
    "    for i in range(len(x_data)):\n",
    "        end_point = min(len(x_data[i]), max_len)\n",
    "        res[i][:end_point] = x_data[i][:end_point]\n",
    "    return res\n",
    "\n",
    "encoded_train = pad_truncate(encoded_train, max_len)\n",
    "encoded_test = pad_truncate(encoded_test, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습, 검증, 테스트 데이터\n",
    "\n",
    "학습하는 동안 우리의 모델이 제대로 학습되었는지 평가하기 위해, 학습 데이터에서 일부를 떼어내어 검증(validation) 데이터 준비합니다.\n",
    "\n",
    "학습된 모델의 중간 평가 및 학습의 중단도 학습의 일부이기 때문에, 검증 데이터를 테스트 데이터로 대체하거나 추출하는 실수는 하지 않도록 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encoded_train\n",
    "X_test = encoded_test\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "\n",
    "# 검증데이터 생성\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------학습 데이터의 비율-----------')\n",
    "print(f'부정 리뷰 = {round(np.sum(y_train==0)/len(y_train) * 100,3)}%')\n",
    "print(f'긍정 리뷰 = {round(np.count_nonzero(y_train)/len(y_train) * 100,3)}%')\n",
    "print('--------검증 데이터의 비율-----------')\n",
    "print(f'부정 리뷰 = {round(np.sum(y_valid==0)/len(y_valid) * 100,3)}%')\n",
    "print(f'긍정 리뷰 = {round(np.count_nonzero(y_valid)/len(y_valid) * 100,3)}%')\n",
    "print('--------테스트 데이터의 비율-----------')\n",
    "print(f'부정 리뷰 = {round(np.sum(y_test==0)/len(y_test) * 100,3)}%')\n",
    "print(f'긍정 리뷰 = {round(np.count_nonzero(y_test)/len(y_test) * 100,3)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('첫번째 샘플의 길이 :', len(X_train[0]))\n",
    "print('첫번째 샘플 :', X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM을 이용한 영화 리뷰 분류\n",
    "\n",
    "우리가 사용하게될 신경망, LSTM에 들어가기 앞서 그 대분류인 순환신경망에 대해서 먼저 알아봅시다.\n",
    "\n",
    "## 순환신경망 (RNN)\n",
    "\n",
    "순환신경망은 입력 또는 출력에 차례(=step)가 있고, 차례로 입/출력을 처리할 때 자기 자신의 상태를 다음 차례의 자기 자신에게 전달하게끔 한 신경망입니다.\n",
    "\n",
    "![image.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/RNN.png?raw=true)\n",
    "\n",
    "전달된 자기 자신의 상태를 이용하여, RNN은 과거의 정보를 추가적으로 가공하거나 활용하여 이후에 있을 출력 예측에 활용합니다.\n",
    "\n",
    "이러한 특징 때문에 순환신경망은 시간에 따른 데이터 (시계열데이터)나 텍스트와 같은 순서가 있는 데이터를 다루는데 적합한 구조입니다.\n",
    "\n",
    "자연어 처리의 경우, 각 스텝의 입력은 그 토큰에 대응하는 벡터가 됩니다.\n",
    "\n",
    "## LSTM\n",
    "\n",
    "Long Short-term Memory(LSTM, 장단기메모리)는 순환신경망(RNN)의 일종으로,\n",
    "\n",
    "장기기억과 단기기억을 분리하여 시간에 감에 따라서 전달하는 신경망입니다.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "![image-2.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/LSTM.png?raw=true)\n",
    "\n",
    "(출처: [Guillaume Chevalier](https://guillaume-chevalier.com/)의 LSTM 그림.)\n",
    "\n",
    "기존 RNN에서는 멀리 떨어진 차례간의 정보전달이 잘 안되고 학습이 더딘 문제가 있었으며\n",
    "\n",
    "이를 해결하고자 장기기억 상태를 가진 LSTM이 탄생하게 되었습니다.\n",
    "\n",
    "아래 예제부터는 이 LSTM을 사용하여 영화 리뷰에 대한 긍정과 부정을 분류합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 사용 장치 설정, gpu (cuda) or cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 Tensor 형으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "X_valid = torch.tensor(X_valid)\n",
    "X_test = torch.tensor(X_test)\n",
    "\n",
    "y_train = torch.tensor(y_train)\n",
    "y_valid = torch.tensor(y_valid)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "print(f\"총 학습 배치의 수 : {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 모델 구축 \n",
    "\n",
    "텍스트 분류를 위한 LSTM 모델의 구조는 다음과 같습니다.\n",
    "\n",
    "![image.png](https://github.com/xaiseung/NLP_Policy_test/blob/main/images/Classification.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, word_emb=None, pad_idx=None):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            vocab_size: 사전에 들어간 단어의 개수입니다.\n",
    "            embedding_dim: 워드 임베딩의 차원 크기입니다.\n",
    "            hidden_dim: 신경망 은닉층 차원 크기입니다.\n",
    "            output_dim: 최종 출력 차원 크기입니다. 분류의 경우, 레이블의 개수가 됩니다.\n",
    "            word_emb: 미리 학습된 단어 임베딩을 전달하여 이로 초기화합니다.\n",
    "            pad_idx: 패딩의 index입니다. 지정시 해당 index의 임베딩은 학습이 이루어지지 않습니다.\n",
    "        \"\"\"\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        # 워드 임베딩\n",
    "        if word_emb is not None:\n",
    "            embedding_dim = word_emb.shape[1]\n",
    "            # 정규화\n",
    "            word_emb = (word_emb - np.mean(word_emb, axis=0)) / (np.std(word_emb, axis=0) + 1e-6)\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(word_emb).to(torch.float), padding_idx=pad_idx)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Feed-forword\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        bsz, seq_len = x.shape\n",
    "        embedded = self.embedding(x) # (b, l, emb_dim)\n",
    "        if self.pad_idx is not None:\n",
    "            # padding 토큰의 마지막 위치를 찾습니다.\n",
    "            pad_mask = (x == self.pad_idx)\n",
    "            pad_idx = torch.argmax(pad_mask.to(torch.int32), dim=1)\n",
    "            # padding 토큰이 없는 경우 (pad_idx==0) 마지막 자리를 가리키도록 예외처리를 합니다\n",
    "            pad_idx[pad_idx==0] = seq_len\n",
    "            last_idx = pad_idx - 1\n",
    "        else:\n",
    "            last_idx = seq_len - 1\n",
    "\n",
    "        # LSTM은 (hidden state, cell state) 튜플을 반환합니다.\n",
    "        # lstm_out: (b, l, h_dim)\n",
    "        # hidden: (1, b, h_dim), cell: (1, b, h_dim)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # padding 토큰이 없는 마지막 위치를 찾습니다.\n",
    "        last_hidden = lstm_out[torch.arange(bsz), last_idx] # (b, h_dim)\n",
    "\n",
    "        logits = self.fc(last_hidden) # (b, out_dim)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 객체를 선언합니다\n",
    "워드 임베딩으로 gensim으로 학습된 `embedding_w`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embedding_dim = embedding_w.shape \n",
    "hidden_dim = 128\n",
    "output_dim = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "model = TextClassifier(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    output_dim,\n",
    "    word_emb=embedding_w,\n",
    "    pad_idx = word2index[\"<pad>\"]\n",
    ")\n",
    "\n",
    "# gpu 사용시 gpu에 모델을 올린다.\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 선언\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 옵티마이저\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 코드 작성\n",
    "\n",
    "모델의 정확도를 측정하는 함수 `calculate_accuracy()`를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, labels):\n",
    "    predicted = torch.argmax(logits, dim=1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_accuracy()`를 이용하여 평가 루프를 수행하는 `evaluate()` 함수를 작성합니다.\n",
    "\n",
    "- `model.eval()`은 모델을 평가 모드로 변환합니다. 신경망의 일부 레이어는 훈련과 평가시에 다르게 작동하는 경우가 있어, 정확한 평가 결과를 얻기 위해서는 평가 모드로 전환해주어야 합니다.\n",
    "\n",
    "- `with torch.no_grad()`는 기울기(gradient)를 구하기 위하여 연산을 기억하는 autograd 엔진을 비활성화합니다. autograd는 평가 결과에는 영향을 주지 않지만 계산 자원을 차지하기 때문에, 비활성화해주는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        loss: 평균 손실함수\n",
    "        acc: 정확도\n",
    "    \"\"\"\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # 예측값\n",
    "            logits = model(X)\n",
    "\n",
    "            # 손실 함수\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            # 정확도와 손실 계산\n",
    "            val_loss += loss.item()\n",
    "            val_correct += calculate_accuracy(logits, y) * y.size(0)\n",
    "            val_total += y.size(0)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss /= len(dataloader)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습\n",
    "\n",
    "이제 학습 루프를 작성해봅시다. 딥러닝 모델을 훈련하고 검증하는 과정을 반복하여, 검증 손실이 개선될 때 마다 모델의 가중치를 저장합니다.\n",
    "결과적으로 학습 중 검증 손실이 가장 낮은 모델의 가중치를 파일로 저장하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    for i, (X, y) in enumerate(train_dataloader):\n",
    "        # 장치 할당\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # 순전파\n",
    "        logits = model(X)\n",
    "\n",
    "        # 손실함수 계산\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # 기존에 누적된 gradient 제거\n",
    "        optimizer.zero_grad()\n",
    "        # 손실함수에 대한 그래디언트 역전파 계산\n",
    "        loss.backward()\n",
    "        # 옵티마이저로 최적화\n",
    "        optimizer.step()\n",
    "\n",
    "        # 학습 지표 계산\n",
    "        train_loss += loss.item()\n",
    "        train_correct += calculate_accuracy(logits, y) * y.size(0)\n",
    "        train_total += y.size(0)\n",
    "\n",
    "        print(f\"({i+1}/{len(train_dataloader)}) epoch {epoch}\\r\", end=\"\")\n",
    "    print(\"\")\n",
    "    train_acc = train_correct / train_total\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # validation\n",
    "    val_loss, val_acc = evaluate(model, valid_dataloader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "    # 검증 손실이 최소일 때 체크포인트 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저장된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "loaded_model = TextClassifier(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    output_dim,\n",
    "    pad_idx = word2index[\"<pad>\"]\n",
    ")\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(\"best_model_checkpoint.pth\"), map_location=device)\n",
    "\n",
    "loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 데이터에 대한 손실함수와 정확도를 출력해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = evaluate(loaded_model, valid_dataloader, criterion, device)\n",
    "\n",
    "print(f'Best model validation loss: {val_loss:.4f}')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 테스트 데이터에 대한 정확도를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_accuracy = evaluate(loaded_model, test_dataloader, criterion, device)\n",
    "\n",
    "print(f'Best model validation accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 테스트\n",
    "\n",
    "텍스트를 입력받아 모델의 예측 결과를 출력하는 함수를 작성해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"부정\", \"긍정\"]\n",
    "def predict(text, model, word2index):\n",
    "    model.eval()\n",
    "\n",
    "    # 토큰화\n",
    "    tokens = tokenizer.morphs(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords] # 불용어 제거\n",
    "    # 사전을 통해 정수 인덱스로\n",
    "    token_indices = text_to_sequences(tokens, word2index)\n",
    "    \n",
    "    # 텐서화\n",
    "    input_tensor = torch.tensor(token_indices, dtype=torch.int32).to(device)\n",
    "\n",
    "    # 모델 입출력\n",
    "    with torch.no_grad():\n",
    "        # logits: (1, 2)\n",
    "        logits = model(input_tensor)\n",
    "    \n",
    "    # 예측한 클래스 인덱스 얻기\n",
    "    pred_idx = torch.argmax(logits, dim=1)\n",
    "    pred_label = labels[pred_idx]\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"아 영화 개꿀잼 ㅋㅋㅋ\"\n",
    "predict(test_input, loaded_model, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"이 영화 정말 쓰레기네\"\n",
    "predict(test_input, loaded_model, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 직접 입력해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = input()\n",
    "predict(test_input, loaded_model, word2index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
